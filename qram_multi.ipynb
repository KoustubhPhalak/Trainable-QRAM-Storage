{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pennylane as qml\n",
    "# from livelossplot import PlotLosses\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_digits().data\n",
    "classes = load_digits().target\n",
    "filter_0 = np.where((classes == 0 ))\n",
    "filter_1 = np.where((classes == 1 ))\n",
    "# data, classes = data[filter], classes[filter]\n",
    "# data, classes = shuffle(data, classes)\n",
    "# data, classes = data[:45], classes[:45]\n",
    "# print(data.shape, classes.shape)\n",
    "num_images = 40\n",
    "x_aux_0, y_0 = data[filter_0], classes[filter_0]\n",
    "x_aux_0, y_0 = x_aux_0[:num_images], y_0[:num_images]\n",
    "x_aux_1, y_1 = data[filter_1], classes[filter_1]\n",
    "x_aux_1, y_1 = x_aux_1[:num_images], y_1[:num_images]\n",
    "x_main = list(range(len(x_aux_0)+len(x_aux_1)))\n",
    "for i in range(len(x_main)):\n",
    "    x_main[i] = np.binary_repr(x_main[i], width=9)\n",
    "    x_main[i] = [int(char) for char in x_main[i]]\n",
    "\n",
    "x_main_0, x_main_1 = x_main[:num_images], x_main[num_images:]\n",
    "\n",
    "x_aux_0_train, y_0_train, x_main_0_train = x_aux_0[:int(0.8*num_images)], y_0[:int(0.8*num_images)], x_main_0[:int(0.8*num_images)]\n",
    "x_aux_1_train, y_1_train, x_main_1_train = x_aux_1[:int(0.8*num_images)], y_1[:int(0.8*num_images)], x_main_1[:int(0.8*num_images)]\n",
    "x_aux_0_test, y_0_test, x_main_0_test = x_aux_0[int(0.8*num_images):], y_0[int(0.8*num_images):], x_main_0[int(0.8*num_images):]\n",
    "x_aux_1_test, y_1_test, x_main_1_test = x_aux_1[int(0.8*num_images):], y_1[int(0.8*num_images):], x_main_1[int(0.8*num_images):]\n",
    "\n",
    "\n",
    "# x_main = (x_main-np.mean(x_main))/np.std(x_main)\n",
    "# x_aux = (x_aux -np.mean(x_aux))/np.std(x_aux)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# DEFINE AUX PQC STRUCTURE AND DECLARE TORCHLAYER FOR IT\n",
    "wires_aux_pqc = 6\n",
    "layers_aux = 3\n",
    "dev = qml.device(\"default.qubit\", wires=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******EPOCH #0 START ******\n",
      "EPOCH LOSS for 0:14.9084672985389\n",
      "EPOCH LOSS for 1:7.196406291031297\n",
      "******EPOCH #0 END ******\n",
      "******EPOCH #1 START ******\n",
      "EPOCH LOSS for 0:11.663903457758355\n",
      "EPOCH LOSS for 1:5.825408537742775\n",
      "******EPOCH #1 END ******\n",
      "******EPOCH #2 START ******\n",
      "EPOCH LOSS for 0:8.937092218294664\n",
      "EPOCH LOSS for 1:4.810793264797238\n",
      "******EPOCH #2 END ******\n",
      "******EPOCH #3 START ******\n",
      "EPOCH LOSS for 0:6.819324052646263\n",
      "EPOCH LOSS for 1:4.13909076500314\n",
      "******EPOCH #3 END ******\n",
      "******EPOCH #4 START ******\n",
      "EPOCH LOSS for 0:5.337946143600788\n",
      "EPOCH LOSS for 1:3.715137814715355\n",
      "******EPOCH #4 END ******\n",
      "******EPOCH #5 START ******\n",
      "EPOCH LOSS for 0:4.404783494341645\n",
      "EPOCH LOSS for 1:3.4091788721521112\n",
      "******EPOCH #5 END ******\n",
      "******EPOCH #6 START ******\n",
      "EPOCH LOSS for 0:3.8504514079387357\n",
      "EPOCH LOSS for 1:3.15026422457641\n",
      "******EPOCH #6 END ******\n",
      "******EPOCH #7 START ******\n",
      "EPOCH LOSS for 0:3.5063037729827244\n",
      "EPOCH LOSS for 1:2.925196364674009\n",
      "******EPOCH #7 END ******\n",
      "******EPOCH #8 START ******\n",
      "EPOCH LOSS for 0:3.257103520860908\n",
      "EPOCH LOSS for 1:2.738149000372055\n",
      "******EPOCH #8 END ******\n",
      "******EPOCH #9 START ******\n",
      "EPOCH LOSS for 0:3.0416586865720383\n",
      "EPOCH LOSS for 1:2.5872549603219412\n",
      "******EPOCH #9 END ******\n",
      "******EPOCH #10 START ******\n",
      "EPOCH LOSS for 0:2.8349485930276175\n",
      "EPOCH LOSS for 1:2.463291044497675\n",
      "******EPOCH #10 END ******\n",
      "******EPOCH #11 START ******\n",
      "EPOCH LOSS for 0:2.633692630824738\n",
      "EPOCH LOSS for 1:2.3568911457963497\n",
      "******EPOCH #11 END ******\n",
      "******EPOCH #12 START ******\n",
      "EPOCH LOSS for 0:2.445486345741194\n",
      "EPOCH LOSS for 1:2.263496779260294\n",
      "******EPOCH #12 END ******\n",
      "******EPOCH #13 START ******\n",
      "EPOCH LOSS for 0:2.2794233839742404\n",
      "EPOCH LOSS for 1:2.1826370498838195\n",
      "******EPOCH #13 END ******\n",
      "******EPOCH #14 START ******\n",
      "EPOCH LOSS for 0:2.139700003099154\n",
      "EPOCH LOSS for 1:2.113970545183035\n",
      "******EPOCH #14 END ******\n",
      "******EPOCH #15 START ******\n",
      "EPOCH LOSS for 0:2.023748643632838\n",
      "EPOCH LOSS for 1:2.0546335738044235\n",
      "******EPOCH #15 END ******\n",
      "******EPOCH #16 START ******\n",
      "EPOCH LOSS for 0:1.9248172345655\n",
      "EPOCH LOSS for 1:2.0003380045197203\n",
      "******EPOCH #16 END ******\n",
      "******EPOCH #17 START ******\n",
      "EPOCH LOSS for 0:1.8365165558094922\n",
      "EPOCH LOSS for 1:1.9485855688559623\n",
      "******EPOCH #17 END ******\n",
      "******EPOCH #18 START ******\n",
      "EPOCH LOSS for 0:1.7558176278088282\n",
      "EPOCH LOSS for 1:1.9003243062449762\n",
      "******EPOCH #18 END ******\n",
      "******EPOCH #19 START ******\n",
      "EPOCH LOSS for 0:1.6829035481834957\n",
      "EPOCH LOSS for 1:1.8582853143079714\n",
      "******EPOCH #19 END ******\n",
      "****\n",
      "\n",
      "\n",
      "\n",
      "****\n",
      "******EPOCH #0 START ******\n",
      "EPOCH LOSS for 0:1.6198995496812008\n",
      "EPOCH LOSS for 1:1.8217325950861518\n",
      "******EPOCH #0 END ******\n",
      "******EPOCH #1 START ******\n",
      "EPOCH LOSS for 0:1.5719203247791045\n",
      "EPOCH LOSS for 1:1.7957421176214792\n",
      "******EPOCH #1 END ******\n",
      "******EPOCH #2 START ******\n",
      "EPOCH LOSS for 0:1.5344612553978456\n",
      "EPOCH LOSS for 1:1.774536607349188\n",
      "******EPOCH #2 END ******\n",
      "******EPOCH #3 START ******\n",
      "EPOCH LOSS for 0:1.503954152372732\n",
      "EPOCH LOSS for 1:1.7555630574286507\n",
      "******EPOCH #3 END ******\n",
      "******EPOCH #4 START ******\n",
      "EPOCH LOSS for 0:1.4783446834868113\n",
      "EPOCH LOSS for 1:1.7373362684193012\n",
      "******EPOCH #4 END ******\n",
      "******EPOCH #5 START ******\n",
      "EPOCH LOSS for 0:1.4564823395368853\n",
      "EPOCH LOSS for 1:1.7194593379124892\n",
      "******EPOCH #5 END ******\n",
      "******EPOCH #6 START ******\n",
      "EPOCH LOSS for 0:1.4376444679461746\n",
      "EPOCH LOSS for 1:1.702164738856343\n",
      "******EPOCH #6 END ******\n",
      "******EPOCH #7 START ******\n",
      "EPOCH LOSS for 0:1.4212715994445413\n",
      "EPOCH LOSS for 1:1.685792569289257\n",
      "******EPOCH #7 END ******\n",
      "******EPOCH #8 START ******\n",
      "EPOCH LOSS for 0:1.4068755032009328\n",
      "EPOCH LOSS for 1:1.6705014236422908\n",
      "******EPOCH #8 END ******\n",
      "******EPOCH #9 START ******\n",
      "EPOCH LOSS for 0:1.3940284707501052\n",
      "EPOCH LOSS for 1:1.656242775941639\n",
      "******EPOCH #9 END ******\n",
      "******EPOCH #10 START ******\n",
      "EPOCH LOSS for 0:1.3823679531412143\n",
      "EPOCH LOSS for 1:1.642872823562479\n",
      "******EPOCH #10 END ******\n",
      "******EPOCH #11 START ******\n",
      "EPOCH LOSS for 0:1.371597636354606\n",
      "EPOCH LOSS for 1:1.6302694588760223\n",
      "******EPOCH #11 END ******\n",
      "******EPOCH #12 START ******\n",
      "EPOCH LOSS for 0:1.3614923804670191\n",
      "EPOCH LOSS for 1:1.6183883491513407\n",
      "******EPOCH #12 END ******\n",
      "******EPOCH #13 START ******\n",
      "EPOCH LOSS for 0:1.3519037658795403\n",
      "EPOCH LOSS for 1:1.6072476274039744\n",
      "******EPOCH #13 END ******\n",
      "******EPOCH #14 START ******\n",
      "EPOCH LOSS for 0:1.342752558385304\n",
      "EPOCH LOSS for 1:1.5968650575075989\n",
      "******EPOCH #14 END ******\n",
      "******EPOCH #15 START ******\n",
      "EPOCH LOSS for 0:1.3340009200018672\n",
      "EPOCH LOSS for 1:1.5871988022273498\n",
      "******EPOCH #15 END ******\n",
      "******EPOCH #16 START ******\n",
      "EPOCH LOSS for 0:1.325616473735108\n",
      "EPOCH LOSS for 1:1.578141891122569\n",
      "******EPOCH #16 END ******\n",
      "******EPOCH #17 START ******\n",
      "EPOCH LOSS for 0:1.317550525921191\n",
      "EPOCH LOSS for 1:1.5695745196526558\n",
      "******EPOCH #17 END ******\n",
      "******EPOCH #18 START ******\n",
      "EPOCH LOSS for 0:1.3097417740930615\n",
      "EPOCH LOSS for 1:1.561427633353326\n",
      "******EPOCH #18 END ******\n",
      "******EPOCH #19 START ******\n",
      "EPOCH LOSS for 0:1.3021379776813582\n",
      "EPOCH LOSS for 1:1.5536995305171117\n",
      "******EPOCH #19 END ******\n"
     ]
    }
   ],
   "source": [
    "@qml.qnode(dev, interface='torch')\n",
    "def aux_pqc(inputs, params):\n",
    "    qml.AmplitudeEmbedding(features=inputs,normalize=True,wires=range(wires_aux_pqc))\n",
    "    seed = 42\n",
    "    qml.RandomLayers(weights=params, wires=range(wires_aux_pqc),seed=seed)\n",
    "    return qml.probs(wires=range(wires_aux_pqc))\n",
    "\n",
    "weights_aux_0 = {'params':(layers_aux, 20)}\n",
    "weights_aux_1 = {'params':(layers_aux, 20)}\n",
    "\n",
    "qcnn_aux_0 = qml.qnn.TorchLayer(aux_pqc, weights_aux_0, init_method=torch.nn.init.normal_)\n",
    "qcnn_aux_1 = qml.qnn.TorchLayer(aux_pqc, weights_aux_1, init_method=torch.nn.init.normal_)\n",
    "\n",
    "# DEFINE MAIN QRAM PQC STRUCTURE AND DECLARE TORCHLAYER FOR IT\n",
    "wires_qcnn = 9\n",
    "layers_qcnn = 3\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qcnn(inputs, params, weights):\n",
    "    qml.AngleEmbedding(features=inputs, wires=range(wires_qcnn), rotation='Y')\n",
    "    # qml.BasisEmbedding(features=inputs, wires=range(wires_qcnn))\n",
    "    for l in range(layers_qcnn):\n",
    "        cnt = 0\n",
    "        #shape = qml.StronglyEntanglingLayers.shape(n_layers=layers_qcnn, n_wires=wires_qcnn)\n",
    "        #weights = np.random.random(size=shape)\n",
    "        for i in range(wires_qcnn - 1):\n",
    "            qml.RY(params[l*36+cnt],wires=i)\n",
    "            qml.RY(params[l*36+cnt+1],wires=i+1)\n",
    "            qml.CNOT(wires=[i,i+1])\n",
    "            qml.CRZ(params[l*36+cnt+2], wires=[i,i+1])\n",
    "            qml.PauliX(wires=i+1)\n",
    "            qml.CRX(params[l*36+cnt+3],wires=[i,i+1])\n",
    "            cnt = cnt + 4\n",
    "        for i in [wires_qcnn - 1]:\n",
    "            qml.RY(params[l*36+cnt],wires=i)\n",
    "            qml.RY(params[l*36+cnt+1],wires=i-(wires_qcnn - 1))\n",
    "            qml.CNOT(wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.CRZ(params[l*36+cnt+2], wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.PauliX(wires=i-(wires_qcnn - 1))\n",
    "            qml.CRX(params[l*36+cnt+3],wires=[i,i-(wires_qcnn - 1)])\n",
    "            cnt = cnt + 4\n",
    "    qml.StronglyEntanglingLayers(weights=weights, wires=range(wires_qcnn))\n",
    "    return qml.probs(wires=[0,1,2,6,7,8])\n",
    "\n",
    "weights_qcnn_0 = {\"params\":layers_qcnn*36, \"weights\":(layers_qcnn,wires_qcnn,layers_qcnn)}\n",
    "weights_qcnn_1 = {\"params\":layers_qcnn*36, \"weights\":(layers_qcnn,wires_qcnn,layers_qcnn)}\n",
    "\n",
    "qcnn_main_0 = qml.qnn.TorchLayer(qcnn, weights_qcnn_0, init_method=torch.nn.init.normal_)\n",
    "qcnn_main_1 = qml.qnn.TorchLayer(qcnn, weights_qcnn_1, init_method=torch.nn.init.normal_)\n",
    "\n",
    "# DEFINE OVERALL QRAM CLASS CONSISTING OF BOTH AUX PQC AND MAIN PQC\n",
    "class QRAM(torch.nn.Module):\n",
    "    def __init__(self, cls):\n",
    "        super().__init__()\n",
    "        self.cls = cls\n",
    "\n",
    "        if self.cls == 0:\n",
    "            self.qcnn_aux = torch.nn.Sequential(qcnn_aux_0)\n",
    "            self.qcnn_main = torch.nn.Sequential(qcnn_main_0)\n",
    "        elif self.cls == 1:\n",
    "            self.qcnn_aux = torch.nn.Sequential(qcnn_aux_1)\n",
    "            self.qcnn_main = torch.nn.Sequential(qcnn_main_1)\n",
    "    \n",
    "    def forward(self,x_aux,x_main):\n",
    "        ground_truth = self.qcnn_aux(x_aux)\n",
    "        pred = self.qcnn_main(x_main)\n",
    "        return ground_truth, pred\n",
    "\n",
    "qram_0 = QRAM(0).to(DEVICE).float()\n",
    "qram_1 = QRAM(1).to(DEVICE).float()\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(qcnn_main.parameters(),lr=0.001,momentum=0.9,nesterov=True)\n",
    "opt_0 = torch.optim.Adam(qram_0.parameters(),lr=0.01)\n",
    "opt_1 = torch.optim.Adam(qram_1.parameters(),lr=0.01)\n",
    "\n",
    "x_aux_0, x_aux_1 = torch.tensor(x_aux_0).to(DEVICE), torch.tensor(x_aux_1).to(DEVICE)\n",
    "y_0, y_1 = torch.tensor(y_0,dtype=torch.double).to(DEVICE), torch.tensor(y_1,dtype=torch.double).to(DEVICE)\n",
    "x_main_0, x_main_1 = torch.tensor(x_main_0, dtype=torch.float64).to(DEVICE), torch.tensor(x_main_1, dtype=torch.float64).to(DEVICE)\n",
    "\n",
    "# TRAIN AUX PQC AND MAIN PQC TOGETHER\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" START ******\")\n",
    "    epoch_loss_0 = torch.tensor([0], dtype=torch.float64).to(DEVICE)\n",
    "    epoch_loss_1 = torch.tensor([0], dtype=torch.float64).to(DEVICE)\n",
    "    for i in range(x_aux_0.shape[0]//BATCH_SIZE):\n",
    "        bs_counter = 0\n",
    "        opt_0.zero_grad()\n",
    "        opt_1.zero_grad()\n",
    "        loss_0 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "        loss_1 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "        while bs_counter < BATCH_SIZE: \n",
    "            # print(x_main[i*BATCH_SIZE+bs_counter])\n",
    "            # ground_truth = aux_pqc(x_aux[i*BATCH_SIZE+bs_counter])\n",
    "            # pred = qcnn_main(x_main[i*BATCH_SIZE+bs_counter])\n",
    "            ground_truth_0, pred_0  = qram_0(x_aux_0[i*BATCH_SIZE+bs_counter],x_main_0[i*BATCH_SIZE+bs_counter])\n",
    "            ground_truth_1, pred_1  = qram_1(x_aux_1[i*BATCH_SIZE+bs_counter],x_main_1[i*BATCH_SIZE+bs_counter])\n",
    "            # print(pred.tolist())\n",
    "            temp_loss_0 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "            temp_loss_1 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "            a = []\n",
    "            for j in range(len(pred_0)):\n",
    "                temp_loss_0 = temp_loss_0 + loss_function(pred_0[j], ground_truth_0[j])\n",
    "                temp_loss_1 = temp_loss_1 + loss_function(pred_1[j], ground_truth_1[j])\n",
    "                # a.append(torch.abs(pred[j]-ground_truth[j]).item())\n",
    "            loss_0 = loss_0 + temp_loss_0\n",
    "            loss_1 = loss_1 + temp_loss_1\n",
    "\n",
    "            epoch_loss_0 = epoch_loss_0 + loss_0\n",
    "            epoch_loss_1 = epoch_loss_1 + loss_1\n",
    "            # loss = loss + loss_function(pred, y_train[i*BATCH_SIZE+bs_counter])\n",
    "            # print(temp_loss.item())\n",
    "            bs_counter = bs_counter + 1\n",
    "        loss_0 = loss_0/BATCH_SIZE\n",
    "        loss_1 = loss_1/BATCH_SIZE\n",
    "        # loss.requires_grad = True\n",
    "        loss_0.backward()\n",
    "        loss_1.backward()\n",
    "        # for param in qcnn_main.parameters():\n",
    "        #     print(param.grad)\n",
    "        opt_0.step()\n",
    "        opt_1.step()\n",
    "    print(\"EPOCH LOSS for 0:\" + str(epoch_loss_0.item()))\n",
    "    print(\"EPOCH LOSS for 1:\" + str(epoch_loss_1.item()))\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" END ******\")\n",
    "\n",
    "print(\"****\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"****\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param in qram_0.parameters():\n",
    "        param.requires_grad = False\n",
    "        break\n",
    "    for param in qram_1.parameters():\n",
    "        param.requires_grad = False\n",
    "        break\n",
    "\n",
    "# TRAIN ONLY MAIN PQC AND KEEP TRAINED AUX PQC PARAMETERS FIXED\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" START ******\")\n",
    "    epoch_loss_0 = torch.tensor([0], dtype=torch.float64).to(DEVICE)\n",
    "    epoch_loss_1 = torch.tensor([0], dtype=torch.float64).to(DEVICE)\n",
    "    for i in range(x_aux_0.shape[0]//BATCH_SIZE):\n",
    "        bs_counter = 0\n",
    "        opt_0.zero_grad()\n",
    "        opt_1.zero_grad()\n",
    "        loss_0 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "        loss_1 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "        while bs_counter < BATCH_SIZE: \n",
    "            # print(x_main[i*BATCH_SIZE+bs_counter])\n",
    "            # ground_truth = aux_pqc(x_aux[i*BATCH_SIZE+bs_counter])\n",
    "            # pred = qcnn_main(x_main[i*BATCH_SIZE+bs_counter])\n",
    "            ground_truth_0, pred_0  = qram_0(x_aux_0[i*BATCH_SIZE+bs_counter],x_main_0[i*BATCH_SIZE+bs_counter])\n",
    "            ground_truth_1, pred_1  = qram_1(x_aux_1[i*BATCH_SIZE+bs_counter],x_main_1[i*BATCH_SIZE+bs_counter])\n",
    "            # print(pred.tolist())\n",
    "            temp_loss_0 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "            temp_loss_1 = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "            a = []\n",
    "            for j in range(len(pred_0)):\n",
    "                temp_loss_0 = temp_loss_0 + loss_function(pred_0[j], ground_truth_0[j])\n",
    "                temp_loss_1 = temp_loss_1 + loss_function(pred_1[j], ground_truth_1[j])\n",
    "                # a.append(torch.abs(pred[j]-ground_truth[j]).item())\n",
    "            loss_0 = loss_0 + temp_loss_0\n",
    "            loss_1 = loss_1 + temp_loss_1\n",
    "            epoch_loss_0 = epoch_loss_0 + loss_0\n",
    "            epoch_loss_1 = epoch_loss_1 + loss_1\n",
    "            # loss = loss + loss_function(pred, y_train[i*BATCH_SIZE+bs_counter])\n",
    "            # print(temp_loss.item())\n",
    "            bs_counter = bs_counter + 1\n",
    "        loss_0 = loss_0/BATCH_SIZE\n",
    "        loss_1 = loss_1/BATCH_SIZE\n",
    "        # loss.requires_grad = True\n",
    "        loss_0.backward(retain_graph=True)\n",
    "        loss_1.backward(retain_graph=True)\n",
    "        opt_0.step()\n",
    "        opt_1.step()\n",
    "    print(\"EPOCH LOSS for 0:\" + str(epoch_loss_0.item()))\n",
    "    print(\"EPOCH LOSS for 1:\" + str(epoch_loss_1.item()))\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" END ******\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******EPOCH #0 START ******\n",
      "Running Accuracy: 30.0%\n",
      "Running Accuracy: 36.666666666666664%\n",
      "0.23872204243072698\n",
      "0.2796815236585071\n",
      "0.24708049569360846\n",
      "0.24668722543179983\n",
      "0.22386773132751972\n",
      "0.2833725563769177\n",
      "0.2361179943352505\n",
      "0.24751527491849742\n",
      "0.2581197439890497\n",
      "0.2548026345649734\n",
      "0.2652288859997436\n",
      "0.2347286341992053\n",
      "0.2540138132017502\n",
      "0.2617054302610723\n",
      "0.26032937806120227\n",
      "0.2375517657751416\n",
      "Running Accuracy: 22 Total:64\n",
      "Testing Accuracy: 8/36\n",
      "EPOCH LOSS:162.07791598784243\n",
      "******EPOCH #0 END ******\n",
      "******EPOCH #1 START ******\n",
      "Running Accuracy: 55.00000000000001%\n",
      "Running Accuracy: 55.00000000000001%\n",
      "0.24741322762795484\n",
      "0.28024199675231964\n",
      "0.2546319985371585\n",
      "0.24969955628852394\n",
      "0.23359199295762656\n",
      "0.2841429337247328\n",
      "0.2466186927174623\n",
      "0.24948281898806843\n",
      "0.2642272764288999\n",
      "0.2604356449561073\n",
      "0.2698811454096118\n",
      "0.2439638224309039\n",
      "0.26154884953248764\n",
      "0.2652360280981747\n",
      "0.26861565753960154\n",
      "0.24458020103507716\n",
      "Running Accuracy: 33 Total:64\n",
      "Testing Accuracy: 7/36\n",
      "EPOCH LOSS:156.991388713622\n",
      "******EPOCH #1 END ******\n",
      "******EPOCH #2 START ******\n",
      "Running Accuracy: 77.5%\n",
      "Running Accuracy: 73.33333333333333%\n",
      "0.254503695756838\n",
      "0.27990255348616655\n",
      "0.2599627998516561\n",
      "0.2534562150715624\n",
      "0.24312982170289904\n",
      "0.2836087252343803\n",
      "0.25618261573251644\n",
      "0.2515950377792736\n",
      "0.2685477270885907\n",
      "0.26575050033033315\n",
      "0.2731315653605531\n",
      "0.2529055131554936\n",
      "0.2678017096940777\n",
      "0.2690379237702425\n",
      "0.27576783500063295\n",
      "0.2522662419890563\n",
      "Running Accuracy: 44 Total:64\n",
      "Testing Accuracy: 1/36\n",
      "EPOCH LOSS:152.7535699949024\n",
      "******EPOCH #2 END ******\n",
      "******EPOCH #3 START ******\n",
      "Running Accuracy: 90.0%\n",
      "Running Accuracy: 85.0%\n",
      "0.2595452833489054\n",
      "0.27889345394876336\n",
      "0.2629728196343737\n",
      "0.25765991598797083\n",
      "0.2513602512929699\n",
      "0.2829989755426783\n",
      "0.2638209903681089\n",
      "0.25436869634311426\n",
      "0.27146645129259833\n",
      "0.26987238486511184\n",
      "0.2751605720013381\n",
      "0.26052746044946445\n",
      "0.2724966043800463\n",
      "0.2729015815974269\n",
      "0.281064773735897\n",
      "0.2598616874504623\n",
      "Running Accuracy: 51 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:149.0319789053911\n",
      "******EPOCH #3 END ******\n",
      "******EPOCH #4 START ******\n",
      "Running Accuracy: 95.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.2628698980903279\n",
      "0.27745984043305133\n",
      "0.26429303613355515\n",
      "0.2619195826367758\n",
      "0.2579269480111948\n",
      "0.28308359013122564\n",
      "0.2693098239150083\n",
      "0.2581428083926698\n",
      "0.27363603026961864\n",
      "0.27249124601686703\n",
      "0.2764988925818434\n",
      "0.2663534106382631\n",
      "0.27605429454538744\n",
      "0.2766522647189312\n",
      "0.2846829775100967\n",
      "0.26676152239726636\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:145.71710050217806\n",
      "******EPOCH #4 END ******\n",
      "******EPOCH #5 START ******\n",
      "Running Accuracy: 97.5%\n",
      "Running Accuracy: 90.0%\n",
      "0.26512247633340963\n",
      "0.27577888859407496\n",
      "0.26486953191448653\n",
      "0.26580536186123005\n",
      "0.2630195117284327\n",
      "0.28388965703660723\n",
      "0.2729647547873053\n",
      "0.2628455488442196\n",
      "0.2755841962975771\n",
      "0.27363471932727\n",
      "0.277781925379995\n",
      "0.2702312129463221\n",
      "0.27909937462116435\n",
      "0.2799678954896302\n",
      "0.2872481883487173\n",
      "0.27257696056409575\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:142.7352916137719\n",
      "******EPOCH #5 END ******\n",
      "******EPOCH #6 START ******\n",
      "Running Accuracy: 97.5%\n",
      "Running Accuracy: 88.33333333333333%\n",
      "0.2668913991850796\n",
      "0.27395931524632694\n",
      "0.26563933771086007\n",
      "0.26890165257884385\n",
      "0.2670635476169126\n",
      "0.28486204117669783\n",
      "0.27546590116000524\n",
      "0.2679328362370915\n",
      "0.2775306488594874\n",
      "0.2734493965054991\n",
      "0.2794457839376071\n",
      "0.2722147131757656\n",
      "0.28209864624313896\n",
      "0.2823384841634218\n",
      "0.28940445740939097\n",
      "0.27708160658900266\n",
      "Running Accuracy: 53 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:140.07297594939737\n",
      "******EPOCH #6 END ******\n",
      "******EPOCH #7 START ******\n",
      "Running Accuracy: 97.5%\n",
      "Running Accuracy: 88.33333333333333%\n",
      "0.26854494923848665\n",
      "0.27219491390121153\n",
      "0.267272259889079\n",
      "0.27099700873168764\n",
      "0.27039731798870836\n",
      "0.2854775942553852\n",
      "0.2775613793702719\n",
      "0.27277335428811544\n",
      "0.27938518276651453\n",
      "0.2723639782882664\n",
      "0.281568040382394\n",
      "0.27270737385305305\n",
      "0.2851262660017679\n",
      "0.28355177123143815\n",
      "0.2915261518142325\n",
      "0.2803641982649783\n",
      "Running Accuracy: 53 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:137.73533327161317\n",
      "******EPOCH #7 END ******\n",
      "******EPOCH #8 START ******\n",
      "Running Accuracy: 97.5%\n",
      "Running Accuracy: 88.33333333333333%\n",
      "0.2702600123454904\n",
      "0.2706887297691926\n",
      "0.2699984372704292\n",
      "0.2720937444082674\n",
      "0.27316149288922803\n",
      "0.2855897145040456\n",
      "0.27977051292249744\n",
      "0.276923043885049\n",
      "0.28098347958973674\n",
      "0.27095865505153827\n",
      "0.2839618727139893\n",
      "0.2722994463128617\n",
      "0.2879370859352571\n",
      "0.2838950736332503\n",
      "0.2936548944897131\n",
      "0.28276861730658853\n",
      "Running Accuracy: 53 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:135.72277300149295\n",
      "******EPOCH #8 END ******\n",
      "******EPOCH #9 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.27210787279957566\n",
      "0.2694723552664548\n",
      "0.27364012975457463\n",
      "0.27230044195492964\n",
      "0.27540242309410246\n",
      "0.2853592101561947\n",
      "0.28228335297227125\n",
      "0.2801796691057831\n",
      "0.28236231798728634\n",
      "0.2695945887634769\n",
      "0.2864380139250963\n",
      "0.2714324104730679\n",
      "0.2903104289714419\n",
      "0.2838738471767133\n",
      "0.29568814092186435\n",
      "0.2846754941868335\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:134.02192752710835\n",
      "******EPOCH #9 END ******\n",
      "******EPOCH #10 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.2740798080835955\n",
      "0.26842295573058\n",
      "0.2777918971938416\n",
      "0.27180041087720175\n",
      "0.2771409735367092\n",
      "0.28511458879889867\n",
      "0.28499710857446514\n",
      "0.2826197675971292\n",
      "0.28378845356781013\n",
      "0.2683097766289256\n",
      "0.2890024637642759\n",
      "0.2702815303160434\n",
      "0.2922118840635405\n",
      "0.28394289249870935\n",
      "0.29757725888428954\n",
      "0.28638983268710666\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:132.59005669106764\n",
      "******EPOCH #10 END ******\n",
      "******EPOCH #11 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.2760485633281491\n",
      "0.26738691640220097\n",
      "0.281981841779167\n",
      "0.27080794641902817\n",
      "0.2783420732834609\n",
      "0.2851902076471056\n",
      "0.2876028975416315\n",
      "0.2845234035219448\n",
      "0.28552788276887014\n",
      "0.26698608695578585\n",
      "0.29178693850282084\n",
      "0.2688483204482147\n",
      "0.2936764965262606\n",
      "0.2844035702421763\n",
      "0.29932565103597697\n",
      "0.28812006982505683\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:131.35672155773014\n",
      "******EPOCH #11 END ******\n",
      "******EPOCH #12 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.2777733629735036\n",
      "0.2662789335189164\n",
      "0.28576249458253916\n",
      "0.2695035760860903\n",
      "0.27890158205292764\n",
      "0.285786148030814\n",
      "0.28972718910769446\n",
      "0.2861875049238916\n",
      "0.2876165927319824\n",
      "0.2655483093427299\n",
      "0.2948291694702823\n",
      "0.2670935931098475\n",
      "0.2946576709674671\n",
      "0.2853915392414914\n",
      "0.30088903886984997\n",
      "0.28995303969632646\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:130.25050097316725\n",
      "******EPOCH #12 END ******\n",
      "******EPOCH #13 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.27902076107329965\n",
      "0.26510570641656944\n",
      "0.2887894026046609\n",
      "0.26800850387097397\n",
      "0.2787386363157038\n",
      "0.2869102154938808\n",
      "0.29110905924757935\n",
      "0.2877699449636566\n",
      "0.28986846863326043\n",
      "0.2640303949618905\n",
      "0.29798054804465113\n",
      "0.2650081768004837\n",
      "0.2950617737542069\n",
      "0.2868797457608033\n",
      "0.30218050159706916\n",
      "0.2918194106229135\n",
      "Running Accuracy: 54 Total:64\n",
      "Testing Accuracy: 0/36\n",
      "EPOCH LOSS:129.22817127290972\n",
      "******EPOCH #13 END ******\n",
      "******EPOCH #14 START ******\n",
      "Running Accuracy: 100.0%\n",
      "Running Accuracy: 90.0%\n",
      "0.2797134012748305\n",
      "0.2639375620396889\n"
     ]
    }
   ],
   "source": [
    "x_aux_0_train, y_0_train, x_main_0_train = x_aux_0[:int(0.8*num_images)], y_0[:int(0.8*num_images)], x_main_0[:int(0.8*num_images)]\n",
    "x_aux_1_train, y_1_train, x_main_1_train = x_aux_1[:int(0.8*num_images)], y_1[:int(0.8*num_images)], x_main_1[:int(0.8*num_images)]\n",
    "x_aux_0_test, y_0_test, x_main_0_test = x_aux_0[int(0.8*num_images):], y_0[int(0.8*num_images):], x_main_0[int(0.8*num_images):]\n",
    "x_aux_1_test, y_1_test, x_main_1_test = x_aux_1[int(0.8*num_images):], y_1[int(0.8*num_images):], x_main_1[int(0.8*num_images):]\n",
    "\n",
    "# AUGMENT QNN CLASSIFIER TO THE QRAM AND TRAIN THE QNN CLASSIFIER BY KEEPING TRAINED QRAM PARAMETERS FIXED\n",
    "layers_classifier = 5\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_ram_classifier(inputs, params, weights, params2):\n",
    "    qml.AngleEmbedding(features=inputs, wires=range(wires_qcnn), rotation='Y')\n",
    "    # QRAM STRUCTURE\n",
    "    for l in range(layers_qcnn):\n",
    "        cnt = 0\n",
    "        for i in range(wires_qcnn - 1):\n",
    "            qml.RY(params[l*36+cnt],wires=i)\n",
    "            qml.RY(params[l*36+cnt+1],wires=i+1)\n",
    "            qml.CNOT(wires=[i,i+1])\n",
    "            qml.CRZ(params[l*36+cnt+2], wires=[i,i+1])\n",
    "            qml.PauliX(wires=i+1)\n",
    "            qml.CRX(params[l*36+cnt+3],wires=[i,i+1])\n",
    "            cnt = cnt + 4\n",
    "        for i in [wires_qcnn - 1]:\n",
    "            qml.RY(params[l*36+cnt],wires=i)\n",
    "            qml.RY(params[l*36+cnt+1],wires=i-(wires_qcnn - 1))\n",
    "            qml.CNOT(wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.CRZ(params[l*36+cnt+2], wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.PauliX(wires=i-(wires_qcnn - 1))\n",
    "            qml.CRX(params[l*36+cnt+3],wires=[i,i-(wires_qcnn - 1)])\n",
    "            cnt = cnt + 4\n",
    "    qml.StronglyEntanglingLayers(weights=weights, wires=range(wires_qcnn))\n",
    "    # QNN CLASSIFIER STRUCTURE\n",
    "    for l in range(layers_classifier):\n",
    "        cnt = 0\n",
    "        for i in range(wires_qcnn - 1):\n",
    "            qml.RY(params2[l*36+cnt],wires=i)\n",
    "            qml.RY(params2[l*36+cnt+1],wires=i+1)\n",
    "            qml.CNOT(wires=[i,i+1])\n",
    "            qml.CRZ(params2[l*36+cnt+2], wires=[i,i+1])\n",
    "            qml.PauliX(wires=i+1)\n",
    "            qml.CRX(params2[l*36+cnt+3],wires=[i,i+1])\n",
    "            cnt = cnt + 4\n",
    "        for i in [wires_qcnn - 1]:\n",
    "            qml.RY(params2[l*36+cnt],wires=i)\n",
    "            qml.RY(params2[l*36+cnt+1],wires=i-(wires_qcnn - 1))\n",
    "            qml.CNOT(wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.CRZ(params2[l*36+cnt+2], wires=[i,i-(wires_qcnn - 1)])\n",
    "            qml.PauliX(wires=i-(wires_qcnn - 1))\n",
    "            qml.CRX(params2[l*36+cnt+3],wires=[i,i-(wires_qcnn - 1)])\n",
    "            cnt = cnt + 4\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "weights_qram_classifier = {\"params\":layers_qcnn*36, \"weights\":(layers_qcnn,wires_qcnn,layers_qcnn), \"params2\":layers_classifier*36}\n",
    "\n",
    "qram_classifier = qml.qnn.TorchLayer(quantum_ram_classifier, weights_qram_classifier, init_method=torch.nn.init.normal_)\n",
    "\n",
    "optimizer_classifier = torch.optim.Adam(qram_classifier.parameters(),lr=0.01)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    for param in qram_classifier.parameters():\n",
    "        if cnt < 2:\n",
    "            param.requires_grad = False\n",
    "        cnt = cnt + 1\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    flag = 1\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" START ******\")\n",
    "    epoch_loss = torch.tensor([0], dtype=torch.float64).to(DEVICE)\n",
    "    running_acc = 0\n",
    "    testing_acc = 0\n",
    "    cnt_0 = 0\n",
    "    cnt_1 = 0\n",
    "    for i in range((x_aux_0_train.shape[0]+x_aux_1_train.shape[0])//BATCH_SIZE):\n",
    "        bs_counter = 0\n",
    "        optimizer_classifier.zero_grad()\n",
    "        loss = torch.tensor([0], dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "        while bs_counter < BATCH_SIZE: \n",
    "            temp_loss = torch.tensor(0, dtype=torch.float64, requires_grad=True).to(DEVICE)\n",
    "            if flag == 0:\n",
    "                with torch.no_grad():\n",
    "                    cnt = 0\n",
    "                    for param in qram_classifier.parameters():\n",
    "                        if cnt >= 2:\n",
    "                            break\n",
    "                        param = list(qram_0.parameters())[cnt]  \n",
    "                        param.requires_grad = False  \n",
    "                        cnt = cnt + 1\n",
    "                loss = loss + loss_function((qram_classifier(x_main_0_train[cnt_0])+1.0)/2, y_0_train[cnt_0])\n",
    "                temp_loss = loss_function((qram_classifier(x_main_0_train[cnt_0])+1.0)/2, y_0_train[cnt_0])\n",
    "                # print(qram_classifier(x_main_0_train[cnt_0]).item(), (qram_classifier(x_main_0_train[cnt_0]).item()+1.0)/2, y_0_train[cnt_0].item())\n",
    "                cnt_0 = cnt_0 + 1\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    cnt = 0\n",
    "                    for param in qram_classifier.parameters():\n",
    "                        if cnt >= 2:\n",
    "                            break\n",
    "                        param = list(qram_1.parameters())[cnt] \n",
    "                        param.requires_grad = False   \n",
    "                        cnt = cnt + 1            \n",
    "                loss = loss + loss_function((qram_classifier(x_main_1_train[cnt_1])+1.0)/2, y_1_train[cnt_1])\n",
    "                temp_loss = loss_function((qram_classifier(x_main_1_train[cnt_1])+1.0)/2, y_1_train[cnt_1])\n",
    "                # print(qram_classifier(x_main_1_train[cnt_1]).item(), (qram_classifier(x_main_1_train[cnt_1]).item()+1.0)/2, y_1_train[cnt_1].item())\n",
    "                cnt_1 = cnt_1 + 1\n",
    "            # print(out.item(), y[i*BATCH_SIZE+bs_counter].item())\n",
    "            epoch_loss = epoch_loss + loss\n",
    "            if temp_loss.item() < 0.25:\n",
    "                running_acc = running_acc + 1\n",
    "            bs_counter = bs_counter + 1\n",
    "            flag = 1-flag\n",
    "        loss = loss/BATCH_SIZE\n",
    "        loss.backward()\n",
    "        optimizer_classifier.step()\n",
    "        if i % 1 == 0 and i != 0:\n",
    "            print(\"Running Accuracy: \" + str(running_acc/((i+1)*BATCH_SIZE)*100) + \"%\")\n",
    "    cnt_0 = 0\n",
    "    cnt_1 = 0\n",
    "    for i in range(x_aux_0_test.shape[0]+x_aux_1_test.shape[0]):\n",
    "        if i%2==0:\n",
    "            print(loss_function((qram_classifier(x_main_0_test[cnt_0])+1.0)/2, y_0_test[cnt_0]).item())\n",
    "            if loss_function((qram_classifier(x_main_0_test[cnt_0])+1.0)/2, y_0_test[cnt_0]).item() < 0.25:\n",
    "                testing_acc = testing_acc + 1\n",
    "            cnt_0 = cnt_0 + 1    \n",
    "        else:\n",
    "            print(loss_function((qram_classifier(x_main_1_test[cnt_1])+1.0)/2, y_1_test[cnt_1]).item())\n",
    "            if loss_function((qram_classifier(x_main_1_test[cnt_1])+1.0)/2, y_1_test[cnt_1]).item() < 0.25: \n",
    "                testing_acc = testing_acc + 1\n",
    "            cnt_1 = cnt_1 + 1\n",
    "        \n",
    "    print(\"Running Accuracy: \" + str(running_acc), \"Total:\" + str(x_aux_0_train.shape[0]+x_aux_1_train.shape[0]))\n",
    "    print(\"Testing Accuracy: \" + str(testing_acc) + \"/\" + str(x_aux_0_test.shape[0]+x_aux_1_test.shape[0]))\n",
    "    print(\"EPOCH LOSS:\" + str(epoch_loss.item()))\n",
    "    print(\"******EPOCH #\" + str(epoch) + \" END ******\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(qram_0.parameters())[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0d7be43a922baecbfa48cb40f6786081e2158e784387c7c5e6345b107643248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
